# -*- coding: utf-8 -*-
"""R_ML_80:20.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zIHZYQMSUu2TffKeDBz3S9mt5XX_EQiu
"""

!pip install nbstripout
!nbstripout "R_ML_80:20.ipynb"

import pandas as pd
import numpy as np
import re
import string
import nltk
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

nltk.download('stopwords')
nltk.download('wordnet')
import numpy as np
import re
import string
import nltk
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

nltk.download('stopwords')
nltk.download('wordnet')

df = pd.read_csv('/content/Reviews.csv')

df.head()

df.shape

print("\nNull values in each column:")
print(df.isnull().sum())

df.dropna(subset=['Summary', 'Text'], how='all', inplace=True)

df['Summary'] = df['Summary'].fillna('')
df['Text'] = df['Text'].fillna('')

df.drop_duplicates(subset='Text', inplace=True)

print("Unique values in Score:", df['Score'].unique())

sns.countplot(x='Score', data=df)
plt.title("Distribution of Review Scores")
plt.show()

df = df[(df['Score'] >= 1) & (df['Score'] <= 5)]

print(df[['HelpfulnessNumerator', 'HelpfulnessDenominator']].describe())

df = df[df['HelpfulnessNumerator'] <= df['HelpfulnessDenominator']]

df['HelpfulnessRatio'] = df.apply(
    lambda row: row['HelpfulnessNumerator'] / row['HelpfulnessDenominator']
    if row['HelpfulnessDenominator'] != 0 else 0, axis=1
)

plt.figure(figsize=(10, 5))
sns.histplot(df['HelpfulnessRatio'], bins=20, kde=True)
plt.title("Distribution of Helpfulness Ratio")
plt.xlabel("Helpfulness Ratio")
plt.show()

df['TextLength'] = df['Text'].apply(len)

plt.figure(figsize=(10, 5))
sns.histplot(df['TextLength'], bins=30, kde=True)
plt.title("Distribution of Review Text Lengths")
plt.xlabel("Text Length")
plt.show()

def map_sentiment(score):
    if score in [1, 2]:
        return 'negative'
    elif score == 3:
        return 'neutral'
    else:
        return 'positive'

df['sentiment'] = df['Score'].apply(map_sentiment)

from wordcloud import WordCloud

def show_wordcloud(sentiment):
    text = " ".join(df[df['sentiment'] == sentiment]['Text'])
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")
    plt.title(f"Word Cloud - {sentiment.capitalize()} Reviews")
    plt.show()

show_wordcloud('positive')
show_wordcloud('neutral')
show_wordcloud('negative')

from collections import Counter
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')

stop_words = set(stopwords.words('english'))

def get_top_words(sentiment, n=15):
    words = " ".join(df[df['sentiment'] == sentiment]['Text']).lower().split()
    words = [word.strip('.,!?()[]') for word in words if word not in stop_words and len(word) > 3]
    common_words = Counter(words).most_common(n)
    print(f"\nTop {n} words in {sentiment} reviews:")
    for word, count in common_words:
        print(f"{word}: {count}")

get_top_words('positive')
get_top_words('neutral')
get_top_words('negative')

plt.figure(figsize=(8, 5))
sns.heatmap(df[['Score', 'HelpfulnessNumerator', 'HelpfulnessDenominator', 'HelpfulnessRatio', 'TextLength']].corr(), annot=True, cmap='coolwarm')
plt.title("Correlation Matrix")
plt.show()

df['full_review'] = df['Summary'] + " " + df['Text']

import re
import nltk
import string
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# Download the necessary NLTK resources
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('punkt_tab') # Download the punkt_tab resource


def preprocess_text(text):
    # Lowercase
    text = text.lower()

    # Remove URLs
    text = re.sub(r"http\S+|www\S+|https\S+", '', text, flags=re.MULTILINE)

    # Remove punctuation and digits
    text = re.sub(r'[^a-zA-Z\s]', '', text)

    # Tokenize
    tokens = nltk.word_tokenize(text)

    # Remove stopwords
    tokens = [word for word in tokens if word not in stopwords.words('english')]

    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens]

    # Join tokens back to string
    return " ".join(tokens)

from tqdm import tqdm

tqdm.pandas()
df['cleaned_review'] = df['full_review'].progress_apply(preprocess_text)

from sklearn.feature_extraction.text import TfidfVectorizer

# Create TF-IDF matrix
vectorizer = TfidfVectorizer(max_features=5000)
X = vectorizer.fit_transform(df['cleaned_review'])

# Target variable
y = df['sentiment']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

lr = LogisticRegression(max_iter=1000)
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)

print("Logistic Regression Report:\n", classification_report(y_test, y_pred_lr))

from sklearn.naive_bayes import MultinomialNB

nb = MultinomialNB()
nb.fit(X_train, y_train)
y_pred_nb = nb.predict(X_test)

print("Naive Bayes Report:\n", classification_report(y_test, y_pred_nb))

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier()
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

print("Random Forest Report:\n", classification_report(y_test, y_pred_rf))

from sklearn.metrics import accuracy_score

print("LR Accuracy:", accuracy_score(y_test, y_pred_lr))
print("NB Accuracy:", accuracy_score(y_test, y_pred_nb))
print("RF Accuracy:", accuracy_score(y_test, y_pred_rf))

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression

# Define the model
lr = LogisticRegression(max_iter=1000)

# Define parameters to tune
param_grid = {
    'C': [0.01, 0.1, 1, 10],         # Regularization strength
    'solver': ['liblinear', 'lbfgs']
}

# GridSearchCV setup
grid = GridSearchCV(estimator=lr, param_grid=param_grid, cv=5, scoring='accuracy', verbose=2, n_jobs=-1)

# Fit model
grid.fit(X_train, y_train)

# Best model
best_model = grid.best_estimator_

print("Best Parameters:", grid.best_params_)

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Predict with best model
y_pred = best_model.predict(X_test)

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred, labels=['positive', 'neutral', 'negative'])

# Visualization
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['positive', 'neutral', 'negative'])
disp.plot(cmap='Blues')
plt.title("Confusion Matrix - Logistic Regression")
plt.show()

import pickle

# Save TF-IDF Vectorizer
with open('tfidf_vectorizer.pkl', 'wb') as f:
    pickle.dump(vectorizer, f)

# Save the trained model
with open('sentiment_model.pkl', 'wb') as f:
    pickle.dump(best_model, f)

import pickle
import re
import nltk
import ipywidgets as widgets
from IPython.display import display
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

# Load NLTK assets
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Load the saved model and vectorizer
with open('sentiment_model.pkl', 'rb') as f:
    model = pickle.load(f)

with open('tfidf_vectorizer.pkl', 'rb') as f:
    vectorizer = pickle.load(f)

# Preprocessing function
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    text = text.lower()
    text = re.sub(r"http\S+|www\S+|https\S+", '', text, flags=re.MULTILINE)
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    tokens = word_tokenize(text)
    tokens = [word for word in tokens if word not in stop_words]
    tokens = [lemmatizer.lemmatize(word) for word in tokens]
    return " ".join(tokens)

# Create input box and button
text_input = widgets.Textarea(
    placeholder='Enter a customer review...',
    description='Review:',
    layout=widgets.Layout(width='100%', height='120px')
)

predict_button = widgets.Button(description="Predict Sentiment", button_style='success')

output_label = widgets.Label()

def on_button_click(b):
    user_input = text_input.value.strip()
    if user_input:
        cleaned = preprocess_text(user_input)
        transformed = vectorizer.transform([cleaned])
        prediction = model.predict(transformed)[0]
        output_label.value = f"Predicted Sentiment: {prediction.capitalize()}"
    else:
        output_label.value = "Please enter a review before clicking."

predict_button.on_click(on_button_click)

# Display widgets
display(text_input, predict_button, output_label)

